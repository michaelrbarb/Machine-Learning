{
  "name": "Machine Learning",
  "tagline": "JHU Machine Learning Coursera ",
  "body": "Exercise Quality \r\n===================================================\r\n<br/>\r\n\r\n### Synopsis\r\nExercise is becoming an important part of general health for individuals and is of interest to the healthcare community as a whole.  Specifically, tracking the amount of exercise completed in a given time frame has been the focus of most fitness tracking devices. However, the future of exercise tracking will be determining the quality of exercise in addition to the quantity. This study will reference previously recorded exercise data in an attempt to predict quality of exercise from on-body and on-equipment sensor information.\r\n\r\n### Data Processing and Exploration\r\nThe data utilized was sourced from the  [Human Activity Recognition and Weight Lifting Exercises](http://groupware.les.inf.puc-rio.br/har) website.\r\n\r\nThe data was filtered to include only acceleration variables and sliced into a more managable data set.\r\n\r\n```r\r\nwleTrain2<-select(wleTrain,contains(\"accel\"),contains(\"classe\"))\r\nwleTrain3<-select(wleTrain2,-contains(\"var\"))\r\nInTrain<-createDataPartition(y=wleTrain3$classe,p=0.5,list=FALSE)\r\nwleTrain4<-wleTrain3[InTrain,]\r\nwleTest4<-wleTrain3[-InTrain,]\r\nInTrain<-createDataPartition(y=wleTrain4$classe,p=0.7,list=FALSE)\r\nwleTrain5<-wleTrain4[InTrain,]\r\nwleTest5<-wleTrain4[-InTrain,]\r\n```\r\n\r\n\r\nThe exercise dataset was examined and exploratory plots were drawn with R.  The plots that illustrated interesting trends were *accel dumbbell y* vs *accel belt z* variables and *accel belt z* vs *accel dumbbell y* variables as shown below.  Accleration less than about -200 at the belt location in the z direction appear to all be of class E.  Additionally, class A and D show somewhat tight clusters on both plots.  Due to these observations classification and random forest models were deemed the most approriate model for the data.\r\n<br/>\r\n<img src=\"unnamed-chunk-3-1.png\" title=\"\" alt=\"\" style=\"display: block; margin: auto;\" /><img src=\"unnamed-chunk-3-2.png\" title=\"\" alt=\"\" style=\"display: block; margin: auto;\" />\r\n\r\n### Model Training and Testing\r\nThe rpart function from the caret package was used for prediction utilizing classification methodology.  Cross validation was implemented with the a K-fold of 5, a value not too big nor small which will keep bias and variance between the predicted and actual values within reason.  The R code and results are below.\r\n\r\n```r\r\nset.seed(33833)\r\nmodFitrpart<-train(classe~.,method='rpart',data=wleTrain5,trControl=trainControl(method=\"cv\",number=5))\r\nprint(modFitrpart$finalModel)\r\n```\r\n\r\n```\r\n## n= 13737 \r\n## \r\n## node), split, n, loss, yval, (yprob)\r\n##       * denotes terminal node\r\n## \r\n##  1) root 13737 9831 A (0.28 0.19 0.17 0.16 0.18)  \r\n##    2) accel_belt_z>=-186.5 13035 9137 A (0.3 0.2 0.18 0.17 0.14)  \r\n##      4) accel_arm_x< -270.5 2268  833 A (0.63 0.1 0.14 0.047 0.074) *\r\n##      5) accel_arm_x>=-270.5 10767 8304 A (0.23 0.22 0.19 0.2 0.15)  \r\n##       10) accel_forearm_x>=-104.5 6336 4668 A (0.26 0.26 0.22 0.093 0.17) *\r\n##       11) accel_forearm_x< -104.5 4431 2877 D (0.18 0.18 0.16 0.35 0.13) *\r\n##    3) accel_belt_z< -186.5 702    8 E (0.011 0 0 0 0.99) *\r\n```\r\n\r\n```r\r\nprint(modFitrpart)\r\n```\r\n\r\n```\r\n## CART \r\n## \r\n## 13737 samples\r\n##    16 predictor\r\n##     5 classes: 'A', 'B', 'C', 'D', 'E' \r\n## \r\n## No pre-processing\r\n## Resampling: Cross-Validated (5 fold) \r\n## Summary of sample sizes: 10989, 10991, 10989, 10990, 10989 \r\n## Resampling results across tuning parameters:\r\n## \r\n##   cp          Accuracy   Kappa     \r\n##   0.02366663  0.4374364  0.27526388\r\n##   0.03860238  0.3547352  0.12231086\r\n##   0.06977927  0.3031189  0.02873436\r\n## \r\n## Accuracy was used to select the optimal model using  the largest value.\r\n## The final value used for the model was cp = 0.02366663.\r\n```\r\n\r\n```r\r\npred_rpart<-predict(modFitrpart,wleTest5)\r\ntable(pred_rpart,wleTest5$classe)\r\n```\r\n\r\n```\r\n##           \r\n## pred_rpart    A    B    C    D    E\r\n##          A 1315  790  717  328  485\r\n##          B    0    0    0    0    0\r\n##          C    0    0    0    0    0\r\n##          D  354  349  309  636  260\r\n##          E    5    0    0    0  337\r\n```\r\n\r\n\r\nThe best model had an Accuracy of 44% (56% classification error).  The rpart model was used to predict the test set to validate the error rate from the rpart function. Results from applying the model to the test data is shown above.  The accuracy using rpart on the test model was  **38.88%**; similar value to the rpart generated accuracy indicating that the model only predicts approximately 40% of the correct responses of any given data set.\r\n\r\nThe accuracy and classification error values were less than ideal, therefore a random forest approach was used.  The random forest model was implemented with the caret package and a K-fold value of 5.  The model was validated by applying it to the test set. The out of bag estimate, or out of sample error of the random forest model was estimated at 5.49%.  Additionally, the worst classification error was only 8.88% for classe *B*.  Both values indicate a strong model with high prediction power.\r\n\r\n\r\n```r\r\nset.seed(33833)\r\nmodFit <- train(classe ~ .,method=\"rf\",trControl=trainControl(method=\"cv\",number=5),\r\n                savePredictions = T,data=wleTrain5)\r\nprint(modFit)\r\n```\r\n\r\n```\r\n## Random Forest \r\n## \r\n## 13737 samples\r\n##    16 predictor\r\n##     5 classes: 'A', 'B', 'C', 'D', 'E' \r\n## \r\n## No pre-processing\r\n## Resampling: Cross-Validated (5 fold) \r\n## Summary of sample sizes: 10989, 10991, 10989, 10990, 10989 \r\n## Resampling results across tuning parameters:\r\n## \r\n##   mtry  Accuracy   Kappa    \r\n##    2    0.9384879  0.9221645\r\n##    9    0.9322275  0.9142193\r\n##   16    0.9198515  0.8985570\r\n## \r\n## Accuracy was used to select the optimal model using  the largest value.\r\n## The final value used for the model was mtry = 2.\r\n```\r\n\r\n```r\r\nprint(modFit$finalModel)\r\n```\r\n\r\n```\r\n## \r\n## Call:\r\n##  randomForest(x = x, y = y, mtry = param$mtry, savePredictions = ..1) \r\n##                Type of random forest: classification\r\n##                      Number of trees: 500\r\n## No. of variables tried at each split: 2\r\n## \r\n##         OOB estimate of  error rate: 5.49%\r\n## Confusion matrix:\r\n##      A    B    C    D    E class.error\r\n## A 3769   30   50   54    3  0.03507424\r\n## B  105 2422   84   23   24  0.08878856\r\n## C   39   61 2265   26    5  0.05467446\r\n## D   42   16  104 2085    5  0.07415631\r\n## E    4   33   21   25 2442  0.03287129\r\n```\r\n\r\n```r\r\npred_rf<-predict(modFit,wleTest5)\r\ntable(pred_rf,wleTest5$classe) \r\n```\r\n\r\n```\r\n##        \r\n## pred_rf    A    B    C    D    E\r\n##       A 1616   45   12   28    1\r\n##       B   11 1046   26    8   13\r\n##       C   23   29  972   44    7\r\n##       D   22   10   12  881    9\r\n##       E    2    9    4    3 1052\r\n```\r\n\r\n\r\nThe model was applied to the test set and the classification error was calculated as **5.4%** indicating that a small percentage of the data was indeed incorrectly categorized. The actual prediction table can be viewed above. Additionally, the calculated classification error is similar to the value generated by the random forest R function thereby validating our model was constructed correctly. The random forest model is superior to the rpart classifcation model used above and was utilized as the final model for quiz 4.\r\nA characteristic of the random forest model worth noting is that after examining variable importance it is clear that the *accel dumbbel y*, *accel belt z* and *accel dumbbel z* are the top variables in predicting the exercise classe which could prove useful in future studies or data collection for exercise tracking devices.\r\n<br/>\r\n<img src=\"unnamed-chunk-8-1.png\" title=\"\" alt=\"\" style=\"display: block; margin: auto;\" />\r\n\r\n### Conclusion\r\nExercise data and the method at which we track and report that information will continue to be important to the health conscious person.  Results from human activity recognition experiments will help to further development of fitness trackers thanks to machine learning techniques employed by data scientists. The decision trees and random forests models will be important tools to use in future development of health and fitness trackers as evidenced by the weight lifting exercise data examined in this study. \r\n\r\n\r\n",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}